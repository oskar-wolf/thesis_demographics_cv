{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15a102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Processing 29214 person crops...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29214/29214 [2:11:11<00:00,  3.71it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Enhanced face crops saved to: ../data/crops_face/20251105/1_cafe_pos_person_faces_yunet\n",
      "ðŸ“¸ Total cropped faces: 7625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# LIGHT PREPROCESSING + FACE DETECTION PIPELINE \n",
    "# (Linked with AccoID & Timestamp)\n",
    "# ===============================================\n",
    "# This notebook processes earlier YOLO-based \"close-up person crops\" and performs:\n",
    "# 1. Controlled image enhancement specifically tuned for CCTV footage.\n",
    "# 2. Face detection using a backend suitable for non-frontal, low-quality, or angled imagery.\n",
    "# 3. Metadata recovery (AccoID + timestamp) from filenames, preserving linkage between\n",
    "#    transaction time and detected faces.\n",
    "# 4. Export of clean, normalized face crops prepared for DeepFace age/gender inference.\n",
    "#\n",
    "# This stage is critical for:\n",
    "# - Maximizing facial detection accuracy under CCTV conditions.\n",
    "# - Preserving temporal and transactional linkage required for the thesis dataset.\n",
    "# - Producing consistent face crops suitable for downstream neural models.\n",
    "from deepface import DeepFace\n",
    "import cv2, os, re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# INPUT_DIR contains YOLO-generated person crops (full upper-body crops).\n",
    "# OUTPUT_DIR stores face-only crops with enhanced clarity.\n",
    "INPUT_DIR = \"../data/crops_person/cafe_POS_person_crops_2025_11_05\"\n",
    "OUTPUT_DIR = \"../data/crops_face/20251105/1_cafe_pos_person_faces_yunet\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# DETECTOR:\n",
    "# - YuNet is a lightweight CNN face detector optimized for real-time systems\n",
    "# - Performs significantly better than traditional Haar cascades under CCTV conditions\n",
    "# - Handles rotations, partial occlusions, and varying illumination well.\n",
    "DETECTOR = \"yunet\"\n",
    "\n",
    "# ENFORCE_DETECTION:\n",
    "# - DeepFace raises an exception if no face is detected when this is True.\n",
    "# - This ensures no silent failures where frames pass undetected.\n",
    "ENFORCE_DETECTION = True\n",
    "\n",
    "# MIN_CONFIDENCE:\n",
    "# - YuNet sometimes returns low-confidence detections in noisy CCTV frames.\n",
    "# - A threshold of 0.65 balances recall (not missing faces) with precision (avoiding false positives).\n",
    "MIN_CONFIDENCE = 0.65\n",
    "\n",
    "# FACE SIZE NORMALIZATION:\n",
    "# - TARGET_MIN ensures extremely small crops don't enter the pipeline.\n",
    "# - TARGET_MAX prevents excessively large crops from destabilizing the model input.\n",
    "# - All faces become scale-normalized, stabilizing age/gender predictions.\n",
    "TARGET_MIN = 192\n",
    "TARGET_MAX = 384\n",
    "\n",
    "# EXPAND_RATIO:\n",
    "# - Expands the bounding box around the detected face.\n",
    "# - This preserves contextual cues (forehead, hairline, chin shape) \n",
    "#   that improve age estimation reliability.\n",
    "EXPAND_RATIO = 0.5\n",
    "\n",
    "# === LIGHT IMAGE PREPROCESSING ===\n",
    "# CCTV footage is often low-light, noisy, contrast-poor, and compressed.\n",
    "# This preprocessing block applies gentle enhancement without altering facial identity.\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "\n",
    "def preprocess_soft(img):\n",
    "    \"\"\"\n",
    "    Apply subtle enhancement to improve detection robustness:\n",
    "    1. CLAHE on the L-channel (lightness) improves local contrast in dim environments.\n",
    "    2. Gamma correction brightens midtones without blowing out highlights.\n",
    "    3. Mild denoising reduces compression artifacts common in CCTV footage.\n",
    "    This avoids aggressive transformations that would distort the facial structure,\n",
    "    preserving authenticity for downstream demographic inference.\n",
    "    \"\"\"\n",
    "    # Convert to LAB to isolate luminance for CLAHE\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    l = clahe.apply(l)  # boost local contrast\n",
    "    lab = cv2.merge((l, a, b))\n",
    "    img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # Gentle gamma correction\n",
    "    gamma = 1.08\n",
    "    lut = np.array([((i / 255.0) ** (1 / gamma)) * 255 \n",
    "                    for i in np.arange(256)]).astype(\"uint8\")\n",
    "    img = cv2.LUT(img, lut)\n",
    "\n",
    "    # Mild denoising (helps with grainy CCTV motion noise)\n",
    "    img = cv2.fastNlMeansDenoisingColored(img, None, 3, 3, 5, 15)\n",
    "    return img\n",
    "\n",
    "# === FILENAME PARSING FUNCTION ===\n",
    "def parse_filename_for_info(filename):\n",
    "    \"\"\"\n",
    "    Extracts the AccoID and timestamp embedded during YOLO-person cropping.\n",
    "    Pattern expected:\n",
    "        AccoID_1147705_20251019_083301_person_0.jpg\n",
    "    Why this matters:\n",
    "    - It maintains linkage between the transaction and the shopperâ€™s face.\n",
    "    - It is critical for constructing time-aligned demographic datasets.\n",
    "    If parsing fails, defaults are assigned to avoid pipeline interruption.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"AccoID_(\\d+)_(\\d{8}_\\d{6})\", filename)\n",
    "    if match:\n",
    "        acco_id = match.group(1)\n",
    "        timestamp = match.group(2)\n",
    "    else:\n",
    "        acco_id, timestamp = \"unknown\", \"00000000_000000\"\n",
    "    return acco_id, timestamp\n",
    "\n",
    "# === PROCESSING LOOP ===\n",
    "# Load all person crops in consistent order for deterministic execution.\n",
    "images = sorted([f for f in os.listdir(INPUT_DIR) if f.lower().endswith((\".jpg\", \".png\"))])\n",
    "print(f\"Processing {len(images)} person crops...\")\n",
    "\n",
    "for i, img_name in enumerate(tqdm(images)):\n",
    "    img_path = os.path.join(INPUT_DIR, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        # Skips corrupted or partially written images\n",
    "        continue\n",
    "\n",
    "    # Parse metadata before preprocessing\n",
    "    acco_id, timestamp = parse_filename_for_info(img_name)\n",
    "\n",
    "    # Apply controlled enhancement to maximize face detector robustness\n",
    "    img = preprocess_soft(img)\n",
    "    h_img, w_img = img.shape[:2]\n",
    "\n",
    "    try:\n",
    "        # DeepFace handles both detection + preprocessing internally.\n",
    "        # extract_faces returns one or many faces depending on the frame content.\n",
    "        detections = DeepFace.extract_faces(\n",
    "            img_path=img_path,\n",
    "            detector_backend=DETECTOR,\n",
    "            enforce_detection=ENFORCE_DETECTION,\n",
    "            align=False  # Disable alignment to preserve camera pose geometry\n",
    "        )\n",
    "\n",
    "        if not detections:\n",
    "            continue\n",
    "\n",
    "        for j, face_info in enumerate(detections):\n",
    "            # Confidence filtering to prevent false positives\n",
    "            conf = face_info.get(\"confidence\", 1.0)\n",
    "            if conf < MIN_CONFIDENCE:\n",
    "                continue\n",
    "\n",
    "            # The detector returns bounding box geometry\n",
    "            area = face_info.get(\"facial_area\", {})\n",
    "            if not area:\n",
    "                continue\n",
    "\n",
    "            x, y, w, h = area[\"x\"], area[\"y\"], area[\"w\"], area[\"h\"]\n",
    "\n",
    "            # --- Bounding box expansion ---\n",
    "            # Enlarging ensures the final crop includes:\n",
    "            # - complete forehead (important for age lines)\n",
    "            # - hair shape (gender classifier cue)\n",
    "            # - jawline and chin proportions (age and gender)\n",
    "            dx, dy = int(w * EXPAND_RATIO), int(h * EXPAND_RATIO)\n",
    "            x1, y1 = max(x - dx, 0), max(y - dy, 0)\n",
    "            x2, y2 = min(x + w + dx, w_img), min(y + h + dy, h_img)\n",
    "            face_crop = img[y1:y2, x1:x2]\n",
    "\n",
    "            # --- Adaptive resizing ---\n",
    "            # Ensures consistent face scale across videos and camera angles.\n",
    "            # Prevents overfitting due to resolution variance during model inference.\n",
    "            h_c, w_c = face_crop.shape[:2]\n",
    "            scale = min(\n",
    "                max(TARGET_MIN / min(h_c, w_c), 1.0),\n",
    "                TARGET_MAX / max(h_c, w_c)\n",
    "            )\n",
    "            new_size = (int(w_c * scale), int(h_c * scale))\n",
    "            face_crop = cv2.resize(face_crop, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            # Save output crop using original transactional metadata\n",
    "            out_name = f\"AccoID_{acco_id}_{timestamp}.jpg\"\n",
    "            out_path = os.path.join(OUTPUT_DIR, out_name)\n",
    "            cv2.imwrite(out_path, face_crop, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "\n",
    "    except Exception:\n",
    "        # Fail silently on detection error (DeepFace often raises edge-case exceptions)\n",
    "        continue\n",
    "\n",
    "print(f\"\\nEnhanced face crops saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Total cropped faces: {len(os.listdir(OUTPUT_DIR))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Processing 27340 person crops...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27340/27340 [2:05:01<00:00,  3.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Enhanced face crops saved to: ../data/crops_face/20251107/1_cafe_pos_person_faces_yunet\n",
      "ðŸ“¸ Total cropped faces: 7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# LIGHT PREPROCESSING + FACE DETECTION PIPELINE\n",
    "# (Linked with AccoID & Timestamp)\n",
    "# ===============================================\n",
    "# This script performs face detection on YOLO-generated person crops.\n",
    "# It applies controlled image enhancement to improve facial visibility\n",
    "# under CCTV-specific constraints (low light, compression noise, angle variation).\n",
    "# Each detected face is cropped, padded, resized, and exported with its original\n",
    "# transaction metadata (AccoID + timestamp), ensuring full traceability.\n",
    "#\n",
    "# This prepares clean, normalized inputs for subsequent DeepFace\n",
    "# age/gender inference.\n",
    "from deepface import DeepFace\n",
    "import cv2, os, re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "# INPUT_DIR: directory containing person-level crops extracted with YOLO.\n",
    "# OUTPUT_DIR: final face-only crops ready for inference.\n",
    "INPUT_DIR = \"../data/crops_person/cafe_POS_person_crops_2025_11_07\"\n",
    "OUTPUT_DIR = \"../data/crops_face/20251107/1_cafe_pos_person_faces_yunet\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Face detector selection:\n",
    "# YuNet is used as it handles angled faces, low-resolution CCTV frames,\n",
    "# and partial occlusions more reliably than Haar cascades or SSD.\n",
    "DETECTOR = \"yunet\"\n",
    "\n",
    "# enforce_detection=True ensures DeepFace raises an error if no face is detected.\n",
    "# We catch and ignore these errors to avoid interrupting the pipeline.\n",
    "ENFORCE_DETECTION = True\n",
    "\n",
    "# Minimum confidence for accepting a detected face.\n",
    "MIN_CONFIDENCE = 0.65\n",
    "\n",
    "# Target minimum and maximum dimensions for the resized face crop.\n",
    "# This normalizes scale variation and stabilizes downstream model performance.\n",
    "TARGET_MIN = 192\n",
    "TARGET_MAX = 384\n",
    "\n",
    "# Expand bounding box to include contextual cues:\n",
    "# - forehead, chin, hairline significantly improve age/gender prediction.\n",
    "EXPAND_RATIO = 0.5\n",
    "\n",
    "# ============================================================\n",
    "# LIGHT IMAGE PREPROCESSING\n",
    "# ============================================================\n",
    "# CCTV footage tends to be dim, noisy, compressed, and low contrast.\n",
    "# This preprocessing enhances images WITHOUT altering visual identity.\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "\n",
    "def preprocess_soft(img):\n",
    "    \"\"\"\n",
    "    Apply subtle enhancement on CCTV person crops:\n",
    "    1. CLAHE on the L-channel (LAB space) improves local contrast while preserving colour.\n",
    "    2. Mild gamma correction brightens midtones without distorting highlights.\n",
    "    3. Fast Non-Local Means denoising reduces compression noise typical of CCTV.\n",
    "    These combined steps improve face detection accuracy while preserving integrity\n",
    "    for demographic inference.\n",
    "    \"\"\"\n",
    "    # Convert to LAB to isolate luminance for CLAHE\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    l = clahe.apply(l)\n",
    "    lab = cv2.merge((l, a, b))\n",
    "    img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # Gentle gamma correction (slightly brightens without washing out detail)\n",
    "    gamma = 1.08\n",
    "    lut = np.array([((i / 255.0) ** (1 / gamma)) * 255\n",
    "                    for i in np.arange(256)]).astype(\"uint8\")\n",
    "    img = cv2.LUT(img, lut)\n",
    "\n",
    "    # Mild denoising to clean compression artifacts and low-light noise\n",
    "    img = cv2.fastNlMeansDenoisingColored(img, None, 3, 3, 5, 15)\n",
    "    return img\n",
    "\n",
    "# ============================================================\n",
    "# FILENAME PARSING FUNCTION\n",
    "# ============================================================\n",
    "def parse_filename_for_info(filename):\n",
    "    \"\"\"\n",
    "    Extract AccoID and timestamp embedded in the filename.\n",
    "    The naming convention is inherited from the YOLO frame extraction script:\n",
    "        AccoID_<id>_<YYYYMMDD_HHMMSS>.jpg\n",
    "\n",
    "    Reverse-engineering this metadata preserves the transactional linkage,\n",
    "    which is critical for your thesis dataset (time-aligned demographics).\n",
    "    \"\"\"\n",
    "    match = re.search(r\"AccoID_(\\d+)_(\\d{8}_\\d{6})\", filename)\n",
    "    if match:\n",
    "        acco_id = match.group(1)\n",
    "        timestamp = match.group(2)\n",
    "    else:\n",
    "        # Fallback values in case of unexpected filenames\n",
    "        acco_id, timestamp = \"unknown\", \"00000000_000000\"\n",
    "    return acco_id, timestamp\n",
    "\n",
    "# ============================================================\n",
    "# PROCESSING LOOP\n",
    "# ============================================================\n",
    "# List all person-crop images in sorted order for deterministic execution.\n",
    "images = sorted([f for f in os.listdir(INPUT_DIR) if f.lower().endswith((\".jpg\", \".png\"))])\n",
    "print(f\"Processing {len(images)} person crops...\")\n",
    "\n",
    "for i, img_name in enumerate(tqdm(images)):\n",
    "    img_path = os.path.join(INPUT_DIR, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        # Skip corrupted files or incomplete writes\n",
    "        continue\n",
    "\n",
    "    # Recover transaction metadata from filename\n",
    "    acco_id, timestamp = parse_filename_for_info(img_name)\n",
    "\n",
    "    # Apply gentle enhancement to improve face detectability\n",
    "    img = preprocess_soft(img)\n",
    "    h_img, w_img = img.shape[:2]\n",
    "\n",
    "    try:\n",
    "        # DeepFace extracts one or more faces per image depending on scene complexity.\n",
    "        # align=False preserves original face orientation which is important for real-world CCTV.\n",
    "        detections = DeepFace.extract_faces(\n",
    "            img_path=img_path,\n",
    "            detector_backend=DETECTOR,\n",
    "            enforce_detection=ENFORCE_DETECTION,\n",
    "            align=False\n",
    "        )\n",
    "\n",
    "        if not detections:\n",
    "            continue\n",
    "\n",
    "        for j, face_info in enumerate(detections):\n",
    "\n",
    "            # Skip low-confidence detections to avoid false positives\n",
    "            conf = face_info.get(\"confidence\", 1.0)\n",
    "            if conf < MIN_CONFIDENCE:\n",
    "                continue\n",
    "\n",
    "            # Extract bounding box from DeepFace output\n",
    "            area = face_info.get(\"facial_area\", {})\n",
    "            if not area:\n",
    "                continue\n",
    "\n",
    "            x, y, w, h = area[\"x\"], area[\"y\"], area[\"w\"], area[\"h\"]\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # ADAPTIVE CROP WITH PADDING\n",
    "            # ---------------------------------------------------------\n",
    "            # Expand bounding box to capture:\n",
    "            # - forehead (improves gender/age model accuracy)\n",
    "            # - jawline (critical for demographic classification)\n",
    "            # - hairline and ears (additional context for gender)\n",
    "            dx, dy = int(w * EXPAND_RATIO), int(h * EXPAND_RATIO)\n",
    "            x1, y1 = max(x - dx, 0), max(y - dy, 0)\n",
    "            x2, y2 = min(x + w + dx, w_img), min(y + h + dy, h_img)\n",
    "\n",
    "            face_crop = img[y1:y2, x1:x2]\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # ADAPTIVE RESIZING\n",
    "            # ---------------------------------------------------------\n",
    "            # Ensures consistent scaling across all faces.\n",
    "            # Prevents instability in DeepFace age/gender output.\n",
    "            h, w = face_crop.shape[:2]\n",
    "            scale = min(\n",
    "                max(TARGET_MIN / min(h, w), 1.0),\n",
    "                TARGET_MAX / max(h, w)\n",
    "            )\n",
    "            new_size = (int(w * scale), int(h * scale))\n",
    "            face_crop = cv2.resize(face_crop, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            # Save the processed face with AccoID + timestamp preserved\n",
    "            out_name = f\"AccoID_{acco_id}_{timestamp}.jpg\"\n",
    "            out_path = os.path.join(OUTPUT_DIR, out_name)\n",
    "            cv2.imwrite(out_path, face_crop, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "\n",
    "    except Exception:\n",
    "        # DeepFace may throw unpredictable errors: corrupted images, unreadable crops, etc.\n",
    "        # These are intentionally suppressed to prevent interrupting full-day processing.\n",
    "        continue\n",
    "\n",
    "print(f\"\\nEnhanced face crops saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Total cropped faces: {len(os.listdir(OUTPUT_DIR))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
