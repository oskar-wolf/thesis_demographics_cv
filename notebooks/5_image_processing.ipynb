{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f1a33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment ready\n"
     ]
    }
   ],
   "source": [
    "# --- Core imports ---\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from gfpgan import GFPGANer\n",
    "\n",
    "# --- Display info ---\n",
    "print(\"‚úÖ Environment ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d1b8fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Input folder: ..\\data\\crops_face\\20251105\\2_cafe_pos_faces_realistic\n",
      "üì¶ Output folder: ..\\data\\crops_face\\20251105\\2_cafe_pos_faces_realistic_gfpgan\n",
      "üîß Using FSRCNN: ../models/FSRCNN_x4.pb\n",
      "ü§ñ Using GFPGAN: ../models/GFPGANv1.4.pth\n"
     ]
    }
   ],
   "source": [
    "# === PATH CONFIGURATION ===\n",
    "#SRC_DIR = Path(r\"../data/crops_face/20251105/1_cafe_pos_person_faces_yunet\")\n",
    "SRC_DIR = Path(r\"../data/crops_face/20251105/2_cafe_pos_faces_realistic\")\n",
    "OUT_DIR = Path(r\"../data/crops_face/20251105/2_cafe_pos_faces_realistic_gfpgan\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === MODEL FILES ===\n",
    "FSRCNN_MODEL = r\"../models/FSRCNN_x4.pb\"\n",
    "GFPGAN_MODEL = r\"../models/GFPGANv1.4.pth\"\n",
    "\n",
    "# === UPSCALE SETTINGS ===\n",
    "UPSCALE_FACTOR = 4  # FSRCNN default x4\n",
    "FACE_UPSCALE = 2  # GFPGAN internal upscale\n",
    "\n",
    "print(f\"üìÇ Input folder: {SRC_DIR}\")\n",
    "print(f\"üì¶ Output folder: {OUT_DIR}\")\n",
    "print(f\"üîß Using FSRCNN: {FSRCNN_MODEL}\")\n",
    "print(f\"ü§ñ Using GFPGAN: {GFPGAN_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "745a0ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading FSRCNN super-resolution model...\n",
      "‚úÖ FSRCNN loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"‚è≥ Loading FSRCNN super-resolution model...\")\n",
    "sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "sr.readModel(FSRCNN_MODEL)\n",
    "sr.setModel(\"fsrcnn\", UPSCALE_FACTOR)\n",
    "print(\"‚úÖ FSRCNN loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1d47850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading GFPGAN face restoration model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrosk\\anaconda3\\envs\\face_enhance\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mrosk\\anaconda3\\envs\\face_enhance\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GFPGAN loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Load GFPGAN model ---\n",
    "print(\"‚è≥ Loading GFPGAN face restoration model...\")\n",
    "restorer = GFPGANer(\n",
    "    model_path=GFPGAN_MODEL,\n",
    "    upscale=FACE_UPSCALE,\n",
    "    arch=\"clean\",\n",
    "    channel_multiplier=2,\n",
    "    bg_upsampler=None,\n",
    ")\n",
    "print(\"‚úÖ GFPGAN loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dade714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def enhance_image_humanlike(\n",
    "    img_path,\n",
    "    out_dir,\n",
    "    sr=None,  # FSRCNN super-res model (optional)\n",
    "    restorer=None,  # GFPGAN restorer (optional)\n",
    "    use_fsrcnn=False,\n",
    "    use_gfpgan=True,\n",
    "    blend_alpha=0.45,\n",
    "    sharpen_strength=0.08,\n",
    "    subtle_mode=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    üéØ Flexible CCTV restoration pipeline ‚Äî realistic, natural results.\n",
    "    Automatically handles FSRCNN + GFPGAN (either or both).\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None or img.size == 0:\n",
    "            print(f\"‚ö†Ô∏è Failed to read {img_path}\")\n",
    "            return None\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        small_face_mode = h < 600 or w < 400\n",
    "        base = img.copy()\n",
    "        restored = base.copy()  # ‚úÖ Ensure variable is always defined\n",
    "\n",
    "        # --- Step 1Ô∏è‚É£: FSRCNN Super-Resolution (if enabled) ---\n",
    "        if use_fsrcnn and sr is not None:\n",
    "            try:\n",
    "                upscaled = sr.upsample(img)\n",
    "                upscaled = cv2.GaussianBlur(upscaled, (3, 3), sigmaX=0.4)\n",
    "                base = upscaled\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è FSRCNN failed for {img_path}: {e}\")\n",
    "                base = img.copy()\n",
    "\n",
    "        # --- Step 2Ô∏è‚É£: GFPGAN Restoration (if enabled) ---\n",
    "        if use_gfpgan and restorer is not None:\n",
    "            try:\n",
    "                _, _, restored = restorer.enhance(\n",
    "                    base, has_aligned=False, only_center_face=False, paste_back=True\n",
    "                )\n",
    "                if restored is None or not isinstance(restored, np.ndarray):\n",
    "                    print(f\"‚ö†Ô∏è GFPGAN returned invalid output for {img_path}\")\n",
    "                    restored = base.copy()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è GFPGAN error for {img_path}: {e}\")\n",
    "                restored = base.copy()\n",
    "        else:\n",
    "            restored = base.copy()\n",
    "\n",
    "        # --- Step 3Ô∏è‚É£: Ensure dimensions and channels match ---\n",
    "        if restored.shape[:2] != base.shape[:2]:\n",
    "            # print(f\"‚ö†Ô∏è Shape mismatch for {Path(img_path).name}: base={base.shape}, restored={restored.shape}\")\n",
    "            restored = cv2.resize(\n",
    "                restored, (base.shape[1], base.shape[0]), interpolation=cv2.INTER_CUBIC\n",
    "            )\n",
    "        if len(restored.shape) == 2:  # grayscale fallback\n",
    "            restored = cv2.cvtColor(restored, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # --- Step 4Ô∏è‚É£: Blend Results ---\n",
    "        alpha = blend_alpha if not small_face_mode else min(blend_alpha + 0.1, 0.6)\n",
    "        try:\n",
    "            blended = cv2.addWeighted(base, 1 - alpha, restored, alpha, 0)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Blending failed for {img_path}: {e}\")\n",
    "            blended = restored.copy()\n",
    "\n",
    "        # --- Step 5Ô∏è‚É£: Gentle Sharpening (optional) ---\n",
    "        if subtle_mode and sharpen_strength > 0:\n",
    "            blurred = cv2.GaussianBlur(blended, (0, 0), 2)\n",
    "            final = cv2.addWeighted(\n",
    "                blended, 1 + sharpen_strength, blurred, -sharpen_strength, 0\n",
    "            )\n",
    "        else:\n",
    "            final = blended\n",
    "\n",
    "        # --- Step 6Ô∏è‚É£: Bilateral Filter (natural smoothing) ---\n",
    "        final = cv2.bilateralFilter(final, d=7, sigmaColor=25, sigmaSpace=25)\n",
    "        final = np.clip(final, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # --- Step 7Ô∏è‚É£: Save output ---\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        out_path = os.path.join(out_dir, f\"{Path(img_path).stem}.png\")\n",
    "        cv2.imwrite(out_path, final, [int(cv2.IMWRITE_PNG_COMPRESSION), 3])\n",
    "\n",
    "        return out_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {img_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4278c85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found 7625 images to enhance.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhancing (Realistic Human-like):  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3306/7625 [1:18:04<1:42:00,  1.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÇ Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images to enhance.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m tqdm(images, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnhancing (Realistic Human-like)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     \u001b[43menhance_image_humanlike\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrestorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblend_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# lower = more natural (less GFPGAN)\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharpen_strength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.08\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# subtle, realistic detail\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubtle_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ All enhanced images saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 47\u001b[0m, in \u001b[0;36menhance_image_humanlike\u001b[1;34m(img_path, out_dir, sr, restorer, use_fsrcnn, use_gfpgan, blend_alpha, sharpen_strength, subtle_mode)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_gfpgan \u001b[38;5;129;01mand\u001b[39;00m restorer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m         _, _, restored \u001b[38;5;241m=\u001b[39m \u001b[43mrestorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menhance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aligned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_center_face\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaste_back\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m restored \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(restored, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ö†Ô∏è GFPGAN returned invalid output for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mrosk\\anaconda3\\envs\\face_enhance\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mrosk\\anaconda3\\envs\\face_enhance\\lib\\site-packages\\gfpgan\\utils.py:145\u001b[0m, in \u001b[0;36mGFPGANer.enhance\u001b[1;34m(self, img, has_aligned, only_center_face, paste_back, weight)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_helper\u001b[38;5;241m.\u001b[39mget_inverse_affine(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# paste each restored face to the input image\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     restored_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaste_faces_to_input_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupsample_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbg_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_helper\u001b[38;5;241m.\u001b[39mcropped_faces, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_helper\u001b[38;5;241m.\u001b[39mrestored_faces, restored_img\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mrosk\\anaconda3\\envs\\face_enhance\\lib\\site-packages\\facexlib\\utils\\face_restoration_helper.py:355\u001b[0m, in \u001b[0;36mFaceRestoreHelper.paste_faces_to_input_image\u001b[1;34m(self, save_path, upsample_img)\u001b[0m\n\u001b[0;32m    353\u001b[0m         upsample_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((upsample_img, alpha), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m         upsample_img \u001b[38;5;241m=\u001b[39m inv_soft_mask \u001b[38;5;241m*\u001b[39m pasted_face \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m inv_soft_mask) \u001b[38;5;241m*\u001b[39m upsample_img\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(upsample_img) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m256\u001b[39m:  \u001b[38;5;66;03m# 16-bit image\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     upsample_img \u001b[38;5;241m=\u001b[39m upsample_img\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint16)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "images = (\n",
    "    sorted(SRC_DIR.glob(\"*.png\"))\n",
    "    + sorted(SRC_DIR.glob(\"*.jpg\"))\n",
    "    + sorted(SRC_DIR.glob(\"*.jpeg\"))\n",
    ")\n",
    "print(f\"üìÇ Found {len(images)} images to enhance.\\n\")\n",
    "\n",
    "for img_path in tqdm(images, desc=\"Enhancing (Realistic Human-like)\"):\n",
    "    enhance_image_humanlike(\n",
    "        img_path=img_path,\n",
    "        out_dir=OUT_DIR,\n",
    "        sr=sr,\n",
    "        restorer=restorer,\n",
    "        blend_alpha=0.5,  # lower = more natural (less GFPGAN)\n",
    "        sharpen_strength=0.08,  # subtle, realistic detail\n",
    "        subtle_mode=False,\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ All enhanced images saved to: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94528561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def detect_frontal_faces(\n",
    "    input_dir=\"../data/crops_face/20251107/2_cafe_pos_faces_realistic\",\n",
    "    output_dir=\"../data/crops_face/20251107/3_cafe_pos_frontal_faces\",\n",
    "    cascade_path=\"../models/haarcascade_frontalface_alt2.xml\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Detects and saves only frontal faces using Haar Cascade.\n",
    "    Keeps images where a frontal face is detected, skips the rest.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load Haar Cascade\n",
    "    face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    if face_cascade.empty():\n",
    "        raise IOError(f\"‚ùå Could not load Haar Cascade: {cascade_path}\")\n",
    "\n",
    "    # Get all image files\n",
    "    images = sorted(\n",
    "        [\n",
    "            f\n",
    "            for f in os.listdir(input_dir)\n",
    "            if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))\n",
    "        ]\n",
    "    )\n",
    "    print(f\"üìÅ Found {len(images)} images to process.\")\n",
    "\n",
    "    kept, skipped = 0, 0\n",
    "\n",
    "    for img_name in tqdm(images, desc=\"Detecting frontal faces\"):\n",
    "        img_path = os.path.join(input_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Haar detection (default, sensitive frontal-face detector)\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray, scaleFactor=1.1, minNeighbors=2, minSize=(20, 20)\n",
    "        )\n",
    "\n",
    "        # If any frontal face detected ‚Üí keep\n",
    "        if len(faces) > 0:\n",
    "            out_path = os.path.join(output_dir, img_name)\n",
    "            cv2.imwrite(out_path, img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "            kept += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "\n",
    "    print(f\"\\n‚úÖ Done! {kept} frontal faces saved to: {output_dir}\")\n",
    "    print(f\"üö´ Skipped {skipped} images without frontal detections.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e02bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 7500 images to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting frontal faces: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [34:36<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Done! 5732 frontal faces saved to: ../data/crops_face/20251107/3_cafe_pos_frontal_faces\n",
      "üö´ Skipped 1768 images without frontal detections.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "detect_frontal_faces(\n",
    "    input_dir=\"../data/crops_face/20251107/2_cafe_pos_faces_realistic\",\n",
    "    output_dir=\"../data/crops_face/20251107/3_cafe_pos_frontal_faces\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c2acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_enhance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
