{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75f1a33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment ready\n"
     ]
    }
   ],
   "source": [
    "# --- Core imports ---\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from gfpgan import GFPGANer\n",
    "\n",
    "# --- Display info ---\n",
    "print(\"‚úÖ Environment ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d1b8fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Input folder: ..\\data\\crops_face\\20251107\\1_cafe_pos_person_faces_yunet\n",
      "üì¶ Output folder: ..\\data\\crops_face\\20251107\\2_cafe_pos_faces_realistic\n",
      "üîß Using FSRCNN: ../models/FSRCNN_x4.pb\n",
      "ü§ñ Using GFPGAN: ../models/GFPGANv1.4.pth\n"
     ]
    }
   ],
   "source": [
    "# === PATH CONFIGURATION ===\n",
    "SRC_DIR  = Path(r\"../data/crops_face/20251107/1_cafe_pos_person_faces_yunet\")\n",
    "OUT_DIR  = Path(r\"../data/crops_face/20251107/2_cafe_pos_faces_realistic\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === MODEL FILES ===\n",
    "FSRCNN_MODEL = r\"../models/FSRCNN_x4.pb\"\n",
    "GFPGAN_MODEL = r\"../models/GFPGANv1.4.pth\"\n",
    "\n",
    "# === UPSCALE SETTINGS ===\n",
    "UPSCALE_FACTOR = 4  # FSRCNN default x4\n",
    "FACE_UPSCALE = 2    # GFPGAN internal upscale\n",
    "\n",
    "print(f\"üìÇ Input folder: {SRC_DIR}\")\n",
    "print(f\"üì¶ Output folder: {OUT_DIR}\")\n",
    "print(f\"üîß Using FSRCNN: {FSRCNN_MODEL}\")\n",
    "print(f\"ü§ñ Using GFPGAN: {GFPGAN_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "745a0ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading FSRCNN super-resolution model...\n",
      "‚úÖ FSRCNN loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"‚è≥ Loading FSRCNN super-resolution model...\")\n",
    "sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "sr.readModel(FSRCNN_MODEL)\n",
    "sr.setModel(\"fsrcnn\", UPSCALE_FACTOR)\n",
    "print(\"‚úÖ FSRCNN loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1d47850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading GFPGAN face restoration model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GFPGAN loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Load GFPGAN model ---\n",
    "print(\"‚è≥ Loading GFPGAN face restoration model...\")\n",
    "restorer = GFPGANer(\n",
    "    model_path=GFPGAN_MODEL,\n",
    "    upscale=FACE_UPSCALE,\n",
    "    arch=\"clean\",\n",
    "    channel_multiplier=2,\n",
    "    bg_upsampler=None\n",
    ")\n",
    "print(\"‚úÖ GFPGAN loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dade714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def enhance_image_humanlike(\n",
    "    img_path,\n",
    "    out_dir,\n",
    "    sr=None,                # FSRCNN super-res model (optional)\n",
    "    restorer=None,          # GFPGAN restorer (optional)\n",
    "    use_fsrcnn=True,\n",
    "    use_gfpgan=False,\n",
    "    blend_alpha=0.45,\n",
    "    sharpen_strength=0.08,\n",
    "    subtle_mode=False\n",
    "):\n",
    "    \"\"\"\n",
    "    üéØ Flexible CCTV restoration pipeline ‚Äî realistic, natural results.\n",
    "    Automatically handles FSRCNN + GFPGAN (either or both).\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None or img.size == 0:\n",
    "            print(f\"‚ö†Ô∏è Failed to read {img_path}\")\n",
    "            return None\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        small_face_mode = (h < 600 or w < 400)\n",
    "        base = img.copy()\n",
    "        restored = base.copy()  # ‚úÖ Ensure variable is always defined\n",
    "\n",
    "        # --- Step 1Ô∏è‚É£: FSRCNN Super-Resolution (if enabled) ---\n",
    "        if use_fsrcnn and sr is not None:\n",
    "            try:\n",
    "                upscaled = sr.upsample(img)\n",
    "                upscaled = cv2.GaussianBlur(upscaled, (3, 3), sigmaX=0.4)\n",
    "                base = upscaled\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è FSRCNN failed for {img_path}: {e}\")\n",
    "                base = img.copy()\n",
    "\n",
    "        # --- Step 2Ô∏è‚É£: GFPGAN Restoration (if enabled) ---\n",
    "        if use_gfpgan and restorer is not None:\n",
    "            try:\n",
    "                _, _, restored = restorer.enhance(\n",
    "                    base,\n",
    "                    has_aligned=False,\n",
    "                    only_center_face=False,\n",
    "                    paste_back=True\n",
    "                )\n",
    "                if restored is None or not isinstance(restored, np.ndarray):\n",
    "                    print(f\"‚ö†Ô∏è GFPGAN returned invalid output for {img_path}\")\n",
    "                    restored = base.copy()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è GFPGAN error for {img_path}: {e}\")\n",
    "                restored = base.copy()\n",
    "        else:\n",
    "            restored = base.copy()\n",
    "\n",
    "        # --- Step 3Ô∏è‚É£: Ensure dimensions and channels match ---\n",
    "        if restored.shape[:2] != base.shape[:2]:\n",
    "            #print(f\"‚ö†Ô∏è Shape mismatch for {Path(img_path).name}: base={base.shape}, restored={restored.shape}\")\n",
    "            restored = cv2.resize(restored, (base.shape[1], base.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "        if len(restored.shape) == 2:  # grayscale fallback\n",
    "            restored = cv2.cvtColor(restored, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # --- Step 4Ô∏è‚É£: Blend Results ---\n",
    "        alpha = blend_alpha if not small_face_mode else min(blend_alpha + 0.1, 0.6)\n",
    "        try:\n",
    "            blended = cv2.addWeighted(base, 1 - alpha, restored, alpha, 0)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Blending failed for {img_path}: {e}\")\n",
    "            blended = restored.copy()\n",
    "\n",
    "        # --- Step 5Ô∏è‚É£: Gentle Sharpening (optional) ---\n",
    "        if subtle_mode and sharpen_strength > 0:\n",
    "            blurred = cv2.GaussianBlur(blended, (0, 0), 2)\n",
    "            final = cv2.addWeighted(blended, 1 + sharpen_strength, blurred, -sharpen_strength, 0)\n",
    "        else:\n",
    "            final = blended\n",
    "\n",
    "        # --- Step 6Ô∏è‚É£: Bilateral Filter (natural smoothing) ---\n",
    "        final = cv2.bilateralFilter(final, d=7, sigmaColor=25, sigmaSpace=25)\n",
    "        final = np.clip(final, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # --- Step 7Ô∏è‚É£: Save output ---\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        out_path = os.path.join(out_dir, f\"{Path(img_path).stem}.png\")\n",
    "        cv2.imwrite(out_path, final, [int(cv2.IMWRITE_PNG_COMPRESSION), 3])\n",
    "\n",
    "        return out_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {img_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4278c85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found 7500 images to enhance.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhancing (Realistic Human-like): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [43:16<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All enhanced images saved to: ..\\data\\crops_face\\20251107\\2_cafe_pos_faces_realistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "images = sorted(SRC_DIR.glob(\"*.png\")) + sorted(SRC_DIR.glob(\"*.jpg\")) + sorted(SRC_DIR.glob(\"*.jpeg\"))\n",
    "print(f\"üìÇ Found {len(images)} images to enhance.\\n\")\n",
    "\n",
    "for img_path in tqdm(images, desc=\"Enhancing (Realistic Human-like)\"):\n",
    "    enhance_image_humanlike(\n",
    "        img_path=img_path,\n",
    "        out_dir=OUT_DIR,\n",
    "        sr=sr,\n",
    "        restorer=restorer,\n",
    "        blend_alpha=0.5,       # lower = more natural (less GFPGAN)\n",
    "        sharpen_strength=0.08, # subtle, realistic detail\n",
    "        subtle_mode=False\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ All enhanced images saved to: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94528561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def detect_frontal_faces(\n",
    "    input_dir=\"../data/crops_face/20251107/2_cafe_pos_faces_realistic\",\n",
    "    output_dir=\"../data/crops_face/20251107/3_cafe_pos_frontal_faces\",\n",
    "    cascade_path=\"../models/haarcascade_frontalface_alt2.xml\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Detects and saves only frontal faces using Haar Cascade.\n",
    "    Keeps images where a frontal face is detected, skips the rest.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load Haar Cascade\n",
    "    face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    if face_cascade.empty():\n",
    "        raise IOError(f\"‚ùå Could not load Haar Cascade: {cascade_path}\")\n",
    "\n",
    "    # Get all image files\n",
    "    images = sorted([f for f in os.listdir(input_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
    "    print(f\"üìÅ Found {len(images)} images to process.\")\n",
    "\n",
    "    kept, skipped = 0, 0\n",
    "\n",
    "    for img_name in tqdm(images, desc=\"Detecting frontal faces\"):\n",
    "        img_path = os.path.join(input_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Haar detection (default, sensitive frontal-face detector)\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=2,\n",
    "            minSize=(20, 20)\n",
    "        )\n",
    "\n",
    "        # If any frontal face detected ‚Üí keep\n",
    "        if len(faces) > 0:\n",
    "            out_path = os.path.join(output_dir, img_name)\n",
    "            cv2.imwrite(out_path, img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "            kept += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "\n",
    "    print(f\"\\n‚úÖ Done! {kept} frontal faces saved to: {output_dir}\")\n",
    "    print(f\"üö´ Skipped {skipped} images without frontal detections.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00e02bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 7500 images to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting frontal faces: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [34:36<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Done! 5732 frontal faces saved to: ../data/crops_face/20251107/3_cafe_pos_frontal_faces\n",
      "üö´ Skipped 1768 images without frontal detections.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "detect_frontal_faces(\n",
    "    input_dir=\"../data/crops_face/20251107/2_cafe_pos_faces_realistic\",\n",
    "    output_dir=\"../data/crops_face/20251107/3_cafe_pos_frontal_faces\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c2acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_enhance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
